---
title: "EDDA - Assignment 2 - Group 77"
subtitle: "Dante de Lang, Ignas Krikštaponis and Kamiel Gülpen"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(rstudioapi)
library(lme4)

knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(getActiveDocumentContext()$path))
# round numbers to 3 digits
options(digits = 3)
```


# Exercise 1
If left alone bread will become moldy, rot or decay otherwise.  To investigate the influence of temperature and  humidity  on  this  process,  the  time  to  decay  was  measured  for  18  slices  of  white  bread,  which  were placed in 3 different environments and humidified or not.  The data are given in the file bread.txt, with the first column time to decay in hours, the second column the environment (cold, warm or intermediate temperature) and the third column the humidity.


**a)**The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions.  Present an R-code for this randomization process.
```{r}
data_bread <- read.table(file="data/bread.txt",header=TRUE)

humid <- factor(rep(c("dry","wet"),each = 9))
temp <- factor(rep(c("cold", "intermediate","warm"),times = 6))

data.frame(humid,temp,slices = sample(1:18))
```

**b)**Make two boxplots of hours versus the two factors and two interaction plots (keeping the two factors fixed in turn).
```{r, fig.width = 12, fig.height=6}
par(mfrow=c(1,2))
boxplot(data_bread$hours~data_bread$environment)
boxplot(data_bread$hours~data_bread$humidity)
interaction.plot(data_bread$humidity,data_bread$environment,data_bread$hours)
interaction.plot(data_bread$environment,data_bread$humidity,data_bread$hours)
```


**c)**Perform  an  analysis  of  variance  to  test  for  effect  of  the  factors temperature, humidity, and the interaction. Describe the interaction effect in words.
```{r}
attach(data_bread)
environment=as.factor(environment)
humidity=as.factor(humidity)
dataaov=lm(hours~humidity*environment,data=data_bread)
anova(dataaov)

summary(dataaov)

```
When looking at the two-way anova model we see that it consists of the following terms: $Y_{ijk}$ = $\mu_{ij}$ + $e_{ijk}$ = $\mu + alpha_{i}$ + $\beta_{j}$ + $\gamma_{ij}$ + $e_{eijk}$ We decompose the formula it this way such that $\mu$ is the overall mean, $\alpha_{i}$ and $\beta_{j}$ are the main effect of level i and j of the first factor and second factor respectively and $\gamma_{ij}$ the interaction effect. 

In order to test the effect of the temperature,humidity,  and  the interaction we set up 3 hypotheses which are:
$H_{AB}$: $\gamma_{ij}$ = 0 for every (i, j) (no interactions between factor A and B)

$H_{A}$: $\alpha_{i}$ = 0 for every i (no main effect of factor A)

$H_{B}$:$\beta_{j}$ = 0 for every j (no main effect of factor B)

We use the test statistics $F_{AB}$ for $H_{AB}$, $F_{A}$ for $H_{A}$ and $F_{B}$ for $H_{B}$ where F is the F-distribution. 

To see if the Hypotheses can be rejected we want to look at the probability that P(F>$f_{AB}$), P(F>$f_{A}$) and P(F>$f_{B}$), the bigger the F value the lower the probability that the Hypothesis lays under a F-distribution and therefore the Hypothesis can be rejected.

We see that the humidity has a p-value of 4.3e-06, environment a p-value of 2.5e-10 and the interaction between the two (humidity:environment) shows a p-value of 3.7e-07. This means that humidity, environment and the interaction effect between humidity and environment have a significant influence on the hours, which means we can reject $H_{A}$, $H_{B}$ and $H_{AB}$.

The interaction effect looks at the difference of differences, for example: it looks at the difference in hours for environment = cold and environment = warm for humidity = wet. Then it looks the difference between environment = cold and environment = warm for humidity = dry. It then looks at the difference between those differences and when this difference is high it shows that there is indeed interaction.

**d)** Which of the two factors has the greatest (numerical) influence on the decay?  Is this a good question?

```{r}
# Without interaction
humidity=as.factor(humidity)
environment=as.factor(environment)
dataaov=lm(hours~humidity+environment)
anova(dataaov)
```

When we want to know which factor has the greatest influence we want to use the additive model as used above. This shows a p-value of 0.026 for humidity and a p-value of 3.7e-05 for environemnt. This means that the environment has the greatest influence. 


**e)**  Check the model assumptions by using relevant diagnostic tools.  Are there any outliers?

```{r, fig.width = 12, fig.height=6}
par(mfrow=c(1,2))
dataaov2=lm(hours~humidity*environment,data=data_bread); 
plot(dataaov2, 1)
plot(dataaov2, 2)
```
The qqplot shows a somewhat linear line which means that based on the qqplot we can state that the data is normally distributed. We also looked at the spread of the residuals, which showed that there are three outliers which are number 5, 7 and 8 which can be observed in both plot.


# Exercise 2

A researcher is interested in the time it takes a student to find a certain product on the internet using a search engine. There are three different types of interfaces with the search engine and especially the effect of these interfaces is of importance. There are five different types of students, indicating their level of computer skill (the lower the value of this indicator, the better the computer skill of the corresponding student). Fifteen students are selected; three from each group with a certain level of computer skill. The data is given in the file search.txt. Assume that the experiment was run according to a randomized block design which you make in a). (Beware that the levels of the factors are coded by numbers.)

**a)** Number the selected students 1 to 15 and show how (by using R) the students could be randomized to the interfaces in a randomized block design.

```{r}
interface <- factor(rep(c(1,2,3),each = 5))
skill <- factor(rep(c(1,2,3,4,5),times = 3))
students <- c(1:15)
block <- data.frame(students,skill,interface); block
```

**b)** Test the null hypothesis that the search time is the same for all interfaces. What type of interface does require the longest search time? For which combination of skill level and type of interface is the search time the shortest? Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 3.

```{r}
data_search <- read.table(file="data/search.txt",header=TRUE)
data_search$skill <- as.factor(data_search$skill)
data_search$interface <- as.factor(data_search$interface)

aovsearch = lm(time~interface+skill, data= data_search)

anova(aovsearch)

summary(aovsearch) 

# Estimate interface 3 and skill 3:
Y = 15.01+4.46+3.03+1.8
Y 
```
Looking at the additive ANOVA test we can conclude that there is a significant main effect of the interface. Furthermore, the summary shows that interface three gives the highest alpha parameter value, making the time it takes for this interface the longest. For the shortest search time, interface 1 can be combined with skill levels 1,2 or 3 since all three have the lowest alpha parameter values without being significant. For the estimation of time it takes a typical user of skill level 3 using interface 3 we can calculate Y by summing the estimates and adding the error, giving a time of 24.3 units.

**c)** Check the model assumptions by using relevant diagnostic tools.

```{r, fig.width = 12, fig.height=6}
par(mfrow=c(1,2))
plot(aovsearch,2)
plot(aovsearch,1)
```

As shown in the above QQ-plot and the residuals-fitted plot there are some outliers that raises some doubt about the normality of the data.


**d)** Perform the Friedman test to test whether there is an effect of interface.

```{r}
friedman.test(data_search$time, data_search$interface, data_search$skill)
```
P-value is significant thus $H_0$ is not rejected and therefore there is a significant effect of the interface. 

**e)** Test the null hypothesis that the search time is the same for all interfaces by a one-way ANOVA test, ignoring the variable skill. Is it right/wrong or useful/not useful to perform this test on this dataset?

```{r}
aovsearch = lm(data_search$time~data_search$interface)
anova(aovsearch) 
```
is it not useless also to ignore skill since the time is clearly also depended on this variable, you can not simply ignore such a variable right?

Looking at the p-value of the one-way ANOVA test, we see that is not significant. We could therefore conclude that the interfaces does not have a significant effect on the search time. However, since the data originates from a random block design, it is not correct to use this test since it leaves out important interactions.

# Excercise 3

In a study on the effect of feedingstuffs on lactation a sample of nine cows were fed with two types of food, and their milk production was measured. All cows were fed both types of food, during two periods, with a neutral period in-between to try and wash out carry-over effects. The order of the types of food was randomized over the cows. The observed data can be found in the file cow.txt, where A and B refer to the types of feedingstuffs.

**a)** Test whether the type of feedingstuffs influences milk production using an ordinary "fixed effects" model, fitted with lm. Estimate the difference in milk production.

```{r}
# read data
data <- read.table(file="data/cow.txt",header=TRUE)
data$treatment <- as.factor(data$treatment); data$order <- as.factor(data$order)
data$id <- as.factor(data$id); data$per <- as.factor(data$per)

# perform fixed effects model analysis
fixed_aov <- lm(milk ~ id + per + treatment, data = data)
anova(fixed_aov)
```
From the results of fixed effects model above we see that the p-value for treatment is > 0.05, therefore we can not conclude that there is a significant effect of the treatment.

**b)** Repeat a) and b) by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a mixed effects model.
```{r, warning=FALSE}
attach(data)
mixed_avo <- lmer(milk ~ treatment + order + per + (1|id),REML=FALSE)
mixed_avo_1 <- lmer(milk ~ order + per + (1|id),REML=FALSE)
anova(mixed_avo_1, mixed_avo)
```
The code above performed an ANOVA test between the random effect model with and without treatment in it. From the p-value of >0.05 we can not say that there is significant difference between the two models, therefore there is no significant effect of the treatment.

**c** Study the commands:
```{r, warning=FALSE}
t.test(milk[treatment=="A"],milk[treatment=="B"], paired=TRUE)
```


# Exercise 4

Stochastic models for word counts are used in quantitative studies on literary styles.  Statistical analysis of the counts can, for example, be used to solve controversies about true author ships.  Another example is the analysis of word frequencies in relation to Jane Austen’s novel Sanditon.  At the time Austen died, this novel was only partly completed.  Austen, however, had made a summary for the remaining part.  An admirer of Austen’s work finished the novel, imitating Austen’s style as much as possible.  The file austen.txt contains counts of different words in some of Austen’s novels:  chapters 1 and 3 of Sense and Sensibility (stored in the Sense column), chapters 1, 2 and 3 of Emma (column Emma), chapters 1 and 6 of Sanditon (both written by Austen herself, column Sand1) and chapters 12 and 24 of Sanditon (both written by the admirer,Sand2)

**a)** Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.

The contingency table test for homogeneity is appropriate because we want to know if the fan writer imitates Austen in a good way. This means that we want to test whether or not the different columns of data in the table come from the same population (writer) or not, which would be the case it the fan imitated Austen correctly. The H0 of the contingency table test for homogeneity states that the distribution of the words is the same for the stories.

**b)** Using  the  given  data  set,  investigate  whether  Austen  herself  was  consistent  in  her  different  novels. Where are the main inconsistencies?

```{r}
data=read.table(file="data/austen.txt",header=TRUE)
austen = data[,1:3]
z = chisq.test(austen)
z
residuals(z)
```

She is not inconsistent as the p-value is above 0.05. This means that we cannot reject the H0. She does however have some main inconsistency, which where the words "a", "that" and "without". As can be seen in the residual table above.

```{r}
z = chisq.test(data)
z
residuals(z)
```
The fan is inconsistent as the p-value of the test is below 0.05. Therefore we have to reject the H0 and accept that the distribution of the words in the stories are not the same. Because Austen herself did not have this inconsistency we can say that the inconsistency is caused by the fan writer. The main inconsistencies were for the words "that" and "an". As can be seen in the residual table above.


# Exercise 5
The data in expenses crime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.)  The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad(crime rate per 100000),crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ(number of persons employed in the state) and pop (population of the state in 1000).  In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

**a)** Make some graphical summaries of the data. Investigate the problem of potential and influence points,and the problem of collinearity.
```{r}
data_crime = read.table(file="data/expensescrime.txt",header=TRUE)
regression_data = data_crime[2:7]
pairs(regression_data)

par(mfrow=c(2,3))
plot(data_crime$expend,data_crime$crime)
plot(data_crime$expend,data_crime$bad)
plot(data_crime$expend,data_crime$lawyers)
plot(data_crime$expend,data_crime$employ)
plot(data_crime$expend,data_crime$pop)
```

```{r}
par(mfrow=c(2,3))
cooks_crime = cooks.distance(lm(expend~crime, data = regression_data))
plot(cooks_crime, type="b")
cooks_bad = cooks.distance(lm(expend~bad, data = regression_data))
plot(cooks_bad, type="b")
cooks_lawyers = cooks.distance(lm(expend~lawyers, data = regression_data))
plot(cooks_lawyers, type="b")
cooks_employ = cooks.distance(lm(expend~employ, data = regression_data))
plot(cooks_employ, type="b")
cooks_pop = cooks.distance(lm(expend~pop, data = regression_data))
plot(cooks_pop, type="b")
```
Looking at the above plots we can see that for the variables bad, lawyers, employ and pop there exist potential and influence points. This is shown through the peaks in the cooks distance plots with peaks going above 1. 

```{r}

# Collinearity
round(cor(regression_data),2)
```
Looking at the collinearity table we see that employee and and lawyers are strongly correlated(0.97). Furthermore, we also see that employee and crime rate per 100000 are strongly correlated(0.87). Also for lawyers and crime rate per 100000 it shows that they are strongly correlated(0.83). Lastly we also see a correlation between pop and bad and pop and lawyers and pop and employ.

```{r}
regressionlm=lm(expend~bad+crime+lawyers+employ, data=regression_data)
car::vif(regressionlm)
# We see a value above 5 for lawyers and employees which means we need to take one out

regressionlm=lm(expend~bad+crime+lawyers, data=regression_data)
car::vif(regressionlm)

# Now it looks good
```
**b)** Fit a linear regression model to the data.  Use both the step-up and the step-down method to find thebest model.  If step-up and step-down yield two different models, choose one and motivate your choice.
```{r}

# Step-up method

print("1st level:")
paste("Bad:",round(summary(lm(expend~bad, data=regression_data))$r.squared, 3))
paste("Crime: ", round(summary(lm(expend~crime, data=regression_data))$r.squared, 3))
paste("Lawyers: ", round(summary(lm(expend~lawyers, data=regression_data))$r.squared, 3)) #0.9369
paste("Employ: ", round(summary(lm(expend~employ, data=regression_data))$r.squared, 3), "- selected")#0.954
paste("Pop: ", round(summary(lm(expend~pop, data=regression_data))$r.squared, 3)) # 0.907


print("2nd level: employ +")
paste("Bad:",round(summary(lm(expend~employ+bad, data=regression_data))$r.squared, 3))
paste("Crime: ",round(summary(lm(expend~employ+crime, data=regression_data))$r.squared, 3))
paste("Pop: ",round(summary(lm(expend~employ+pop, data=regression_data))$r.squared, 3))
paste("Lawyers: ",round(summary(lm(expend~employ+lawyers, data=regression_data))$r.squared, 3), "- selected") #0.9631 ==> only significant model


# expend = -1.146e+02 + 2.690e-02*lawyers + 2.976e-02*employ  + error
# Step-down

summary(lm(expend~bad+crime+lawyers+employ + pop, data=regression_data))

summary(lm(expend~lawyers+employ+bad + pop, data=regression_data))

summary(lm(expend~lawyers+employ + bad, data=regression_data))

summary(lm(expend~lawyers+employ , data=regression_data))
```

Step up method:
First we started by fitting a linear model with one explanatory variable. Out of the 5 available variables the employ performed the best according to r-squared value, therefore it was selected for further fitting. Next, another layer of explanatory variables were added to the linear model. Here, only adding Lawyers resulted in a model that had all significant parameters, therefore it was the final model that we chose.

Step down method:


**c)** Check the model assumptions (of the resulting model from b)) by using relevant diagnostic tools.
```{r}
right_plot = lm(expend~lawyers+employ , data=regression_data)
qqnorm(residuals(right_plot))
plot(fitted(right_plot), residuals(right_plot))
plot(right_plot, 1)
plot(right_plot, 2)
```