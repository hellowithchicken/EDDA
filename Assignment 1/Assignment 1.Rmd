---
title: "EDDA - Assignment 1"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(rstudioapi)
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(getActiveDocumentContext()$path))
```


# Exercise 2

## a)

```{r}
n <- m <- 30
mu <- 180
nu <- 175
sd <- 5
grid <- seq(175,185, by=0.25)

power_function<-function(grid,n,m,mu,sd) {
  B <- 1000
  p <- numeric(B)
  G <- length(grid)
  fractions <- numeric(G)
  for (grid_nu in 1:G){
    p <- numeric(B)
    for (b in 1:B){
      x <- rnorm(n,mu,sd)
      y <- rnorm(m,grid[grid_nu],sd)
      p[b] <- t.test(x,y, var.equal = TRUE)[[3]]
    }
    fractions[grid_nu] <- mean(p<0.05)
  }
  return(fractions)
}

fractions_A <- power_function(grid,n,m,mu,sd)

```
## b)

```{r}
n <- m <- 100
mu <- 180
sd <- 5

fractions_B <- power_function(grid,n,m,mu,sd)
```

## c)

```{r}
n <- m <- 30
mu <- 180
sd <- 15

fractions_C <- power_function(grid,n,m,mu,sd)
par(mfrow=c(1,3))
plot(grid,fractions_A)
plot(grid,fractions_B)
plot(grid,fractions_C)
```

## d)
The more datapoints seems to have an influence on the narrowness of the plot. Furthermore, a bigger std gives a more wider distribution of fractions as presented in the plot of C.

# Exercise 3

## a)

```{r}
data<-read.table(file="data/telephone.txt",header=TRUE)
data_tele <- data$Bills
hist(data_tele)
qqnorm(data_tele)
boxplot(data_tele)
```

The data seems oddly distributed with two subpeaks, would have expected a more normally distributed set. Therefore perhaps a good idea if the manager arranges the prices better.

## b)

```{r}
X <- seq(0.01, 0.1, 0.0005)
pvalues <- c()
t <- median(data_tele)
for (x in X){
  B <- 1000
  tstar <- numeric(B)
  n <- length(data_tele)
  
  for (i in 1:B){
    xstar <- rexp(n,x)
    tstar[i] <- median(xstar)
  }
  pl<-sum(tstar<t)/B
  pr<-sum(tstar>t)/B
  p<-2*min(pl,pr)
  pl;pr;p
  pvalues <- c(pvalues,p)
}
pvalues
plot(X, pvalues)
```
There exist a Exp function that fits the hypothesis

## c)
```{r}
B <- 1000
T1 <- median(data_tele)
Tstar <- numeric(B)
for (i in 1:B){
  Xstar <- sample(data_tele,replace=TRUE)
  Tstar[i] <- median(Xstar)
}
Tstar25 <- quantile(Tstar,0.025)
Tstar975 <- quantile(Tstar, 0.975)

T1
c(2*T1-Tstar975, 2*T1-Tstar25)

hist(data_tele, prob=T, ylim=c(0,0.05))
x<-seq(0,max(data_tele),length=1000)
lines(x,exp(x),type="l",col="blue",lwd=2)
```

## d)
```{r}
max_index <- which.max(pvalues)

opt_Lambda <- X[max_index]
```
The variable opt_Lambda is the optimal lambda value with the highest P-value.

## e)
```{r}
bill_bigeq40 <- sum(data_tele>=40)
bill_smal40 <- sum(data_tele<40)

binom.test(bill_bigeq40, length(data_tele),p=0.5)
binom.test(bill_smal40, length(data_tele),p=0.5)

bill_less10 <- sum(data_tele < 10)
bill_less10/length(data_tele)
```


# Exercise 4

## a)

Disregarding the type of drink, test whether the run times before drink and after are correlated.

```{r}
data <- read.table(file="data/run.txt",header=TRUE)
cor(data$before, data$after)
```

Run times before and after the drink seem to be positively correlated.

## b)

```{r}
# calculate differences
data <- data %>% 
  mutate(diff = before - after)

# filter for lemo

lemo <- data %>% 
  filter(drink == "lemo")

t.test(lemo$before, lemo$after, paired = TRUE)

# filter for energy

energy <- data %>% 
  filter(drink == "energy")

t.test(energy$before, energy$after, paired = TRUE)
```

For both energy and soft-drink groups there does not seem to be a significant difference in running times.

## c)

For each pupil compute the time difference between the two running tasks. Test whether these time
differences are effected by the type of drink.

```{r}

# perform t-test

t.test(lemo$diff, energy$diff)

```
The p-value is > 0.05 therefore the means of the two populations are not significantly different.

## d)

Can you think of a plausible objection to the design of the experiment in b) if the main aim was to test
whether drinking the energy drink speeds up the running? Is there a similar objection to the design
of the experiment in c)? Comment on all your fndings in this exercise.

# Exercise 5

## a)

Test whether the distributions of the chicken weights for meatmeal and sunflower groups are different
by performing three tests: the two samples t-test (argue whether the data are paired or not), the
Mann-Whitney test and the Kolmogorov-Smirnov test. Comment on your fndings.

```{r}
# filter for meatmeal

meatmeal <- chickwts %>% 
  filter(feed == "meatmeal") %>% 
  select(weight)

# filter for sunflower

sunflower <- chickwts %>% 
  filter(feed == "sunflower") %>% 
  select(weight)

# check for data normality

qqnorm(meatmeal$weight)
qqnorm(meatmeal$weight)

# perform t-test, the data is not paired

t.test(meatmeal, sunflower)

# Mann-Whitney test

wilcox.test(meatmeal$weight, sunflower$weight)

# Kolmogorov-Smirnov test

ks.test(meatmeal$weight, sunflower$weight)

```

Data in chickwts is not paired as the "treatment" of different feed was applied to different newly-hatched chicks not the same chick. From t-test we can see that the p-values <0.05, therefore the means between the two groups are significantly different. From Mann-Whitney test we can see that p-value is >0.05 therefore we can not conclude that the medians of the two datasets are different. From Kolgomorov-Smirnov test we can not conclude that the means are different.

## b)

Conduct a one-way ANOVA to determine whether the type of feed supplement has an effect on the
weight of the chicks. Give the estimated chick weights for each of the six feed supplements. What is
the best feed supplement?

```{r}
chickaov <- lm(weight~feed, data = chickwts)
# performing one-way ANOVA
anova(chickaov)
#extracting more information
summary_table <- summary(chickaov)
summary_table$coefficients
```
From the results of one-way ANOVA we can see that the p-values is <0.05 therefore we can conclude that the means between all of the feed varieties are significantly different. 
From summary statistics it seems that "sunflower" feed is the feed resulting in highest weight. therefore it is the best.

## c)

Check the ANOVA model assumptions by using relevant diagnostic tools.

```{r}
# check for normality
qqnorm(chickaov$residuals)
# check if the variances are equal
chickwts %>% 
  group_by(feed) %>% 
  summarise(variance = var(weight))
```
From qqplot assumption of normality holds. However the assumption of equal variances does not hold.

## d)

Does the Kruskal-Wallis test arrive at the same conclusion about the effect of feed supplement as the
test in b)? Explain possible differences between conclusions of the Kruskal-Wallis and ANOVA tests.

```{r}
kruskal.test(weight~feed, data = chickwts)
```

With Kruskal-Wallis test we arrive to the same conclusion as with ANOVA. 

