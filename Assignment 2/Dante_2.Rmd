---
title: "EDDA - Assignment 2 - Group 77"
subtitle: "Dante de Lang, Ignas Krikštaponis and Kamiel Gülpen"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(rstudioapi)

knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(getActiveDocumentContext()$path))
# round numbers to 3 digits
options(digits = 3)
```

# Exercise 1

Moldy bread If left alone bread will become moldy, rot or decay otherwise. To investigate the influence of temperature and humidity on this process, the time to decay was measured for 18 slices of white bread, which were placed in 3 different environments and humidified or not. The data are given in the filebread.txt, with the first column time to decay in hours, the second column the environment (cold, warm or intermediate temperature) and the third column the humidity.

**a)** The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions. Present an R-code for this randomization process.

```{r}
humid <- c("dry","wet")
temp <- c("cold", "intermediate","warm")

humidity <- factor(rep(c("dry","wet"),each = 9))
temperature <- factor(rep(c("cold", "intermediate","warm"),times = 6))


combination <- data.frame(humidity,temperature,slices = sample(1:18))
combination
```

**b)** Make two boxplots of hours versus the two factors and two interaction plots (keeping the two factors fixed in turn).

```{r}
data_bread

attach(data_bread)
par(mfrow=c(1,2))
boxplot(hours~environment)
boxplot(hours~humidity)

par(mfrow=c(1,2))
interaction.plot(environment,humidity,hours)
interaction.plot(humidity,environment,hours)
```

**c)** Perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction. Describe the interaction effect in words.

```{r}
aovbread = lm(hours~environment*humidity)
anova(aovbread)
summary(aovbread)
```

**d)** Which of the two factors has the greatest (numerical) influence on the decay? Is this a good question?

```{r}

```

**e)** Check the model assumptions by using relevant diagnostic tools. Are there any outliers?

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(aovbread))
plot(fitted(aovbread),residuals(aovbread))
```


# Exercise 2

A researcher is interested in the time it takes a student to find a certain product on the internet using a search engine. There are three different types of interfaces with the search engine and especially the effect of these interfaces is of importance. There are five different types of students, indicating their level of computer skill (the lower the value of this indicator, the better the computer skill of the corresponding student). Fifteen students are selected; three from each group with a certain level of computer skill. The data is given in the file search.txt. Assume that the experiment was run according to a randomized block design which you make in a). (Beware that the levels of the factors are coded by numbers.)

**a)** Number the selected students 1 to 15 and show how (by using R) the students could be randomized to the interfaces in a randomized block design.

```{r}
interface <- factor(rep(c(1,2,3),each = 5))
skill <- factor(rep(c(1,2,3,4,5),times = 3))
students <- c(1:15)
block <- data.frame(students,skill,interface); block

# REMARK: Do i need to include skill, since only interfaces is mentioned. And perhaps use a random sample instead of just dividing?
```

**b)** Test the null hypothesis that the search time is the same for all interfaces. What type of interface does require the longest search time? For which combination of skill level and type of interface is the search time the shortest? Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 3.

```{r}
data_search <- read.table(file="data/search.txt",header=TRUE)
data_search$skill <- as.factor(data_search$skill)
data_search$interface <- as.factor(data_search$interface)

aovsearch = lm(time~interface+skill, data= data_search)

anova(aovsearch) # The p-value for the interfaces is not significant >0.05 and therefore search time of 2 and 3 is not the same as 1.

summary(aovsearch) 

boxplot(time~interface) # Interface 3 has the longest search time
 # Skill 2 and interface 1 is the fastest

# Estimate interface 3 = 4.46, skill 3 = 3.03, so 3-3 gives:
(4.46+3.03)/2 # 3.75 seconds ???? Still vague how this works


summary(my_life)

```

**c)** Check the model assumptions by using relevant diagnostic tools.

```{r}
par(mfrow=c(1,2))
# qqnorm(residuals(aovsearch))
# plot(fitted(aovsearch),residuals(aovsearch))
plot(aovsearch,2)
plot(aovsearch,1)
```

**d)** Perform the Friedman test tot test whether there is an effect of interface.

```{r}
interaction.plot(interface,skill,time) # Parallel lines indicate no significant interaction effect (slide 6.6)
friedman.test(time,interface,skill) # P-value is significant thus H0 is not rejected and therefore there is no effect of the interface
```

**e)**Test the null hypothesis that the search time is the same for all interfaces by a one-way ANOVA test, ignoring the variable skill. Is it right/wrong or useful/not useful to perform this test on this dataset?

```{r}
aovsearch = lm(data_search$time~data_search$interface)
anova(aovsearch) 
# is it not useless also to ignore skill since the time is clearly also depended on this variable, you can not simply ignore such a variable right?

# While only looking at interface with I/degrees = 2, you can also just perform a t.test, no????

t.test(data_search$time,as.numeric(data_search$interface))
```

# Exercise 3

In a study on the effect of feedingstuffs on lactation a sample of nine cows were fed with two types of food, and their milk production was measured. All cows were fed both types of food, during two periods, with a neutral period in-between to try and wash out carry-over effects. The order of the types of food was randomized over the cows. The observed data can be found in the file cow.txt, where A and B refer to the types of feedingstuffs.

**a)** Test whether the type of feedingstuffs influences milk production using an ordinary "fixed effects" model, fitted with lm. Estimate the difference in milk production.

```{r}
data_cow <- read.table(file="data/cow.txt",header=TRUE)

aovcow <- lm(data_cow$milk~data_cow$per+factor(data_cow$order)+data_cow$id+factor(data_cow$treatment))
anova(aovcow)

summary(aovcow)
```

**b)** Repeat a) and b) by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a mixed effects model.

```{r}
library(lme4)

cow_lmer <- lmer(data_cow$milk~factor(data_cow$treatment)+factor(data_cow$order)+(1|data_cow$id), REML=FALSE)
summary(cow_lmer)
```

**c)** Study the commands:

```{r}
t.test(data_cow$milk[treatment=="A"],data_cow$milk[treatment=="B"],paired=TRUE)
```

Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?

# Exercise 4

Stochastic models for word counts are used in quantitative studies on literary styles. Statistical analysis of the counts can, for example, be used to solve controversies about true authorships. Another example is the analysis of word frequencies in relation to Jane Austen's novel Sanditon. At the time Austen died, this novel was only partly completed. Austen, however, had made a summary for the remaining part. An admirer of Austen's work finished the novel, imitating Austen's style as much as possible. The file austen.txt contains counts of different words in some of Austen's novels: chapters 1 and 3 of Sense and Sensibility (stored in the Sense column), chapters 1, 2 and 3 of Emma (column Emma), chapters 1 and 6 of Sanditon (both written by Austen herself, column Sand1) and chapters 12 and 24 of Sanditon (both written by the admirer, Sand2).

**a)** Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.

```{r}
# Not directly? Because the data is not really a contingency table but more just frequency table, there are not two factors
```

**b)** Using the given data set, investigate whether Austen herself was consistent in her different novels. Where are the main inconsistencies?

```{r}
data_aus <- read.table(file="data/austen.txt",header=TRUE)
data_aus
z <- chisq.test(data_aus[1:3],simulate.p.value=TRUE); z

residuals(z)
# Looking at this table we can state that in the Sanditon book she used more "a" and less "that" then in her earlier work.
```

**c)** Was the admirer successful in imitating Austen's style? Perform a test including all data. If he wasnot successful, where are the differences?

```{r}
z <- chisq.test(data_aus,simulate.p.value=TRUE); z

residuals(z)

# Looking at this table we can see that the admirer uses considerably more "an" and less "that" then Austin's original works.
```

# Exercise 5

The data in expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in \$1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

**a)** Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.

```{r}
data_crime <- read.table(file="data/expensescrime.txt",header=TRUE)

par(mfrow=c(1,3));plot(data_crime)
par(mfrow=c(1,3));plot(data_crime[,c(2,6,7)])
par(mfrow=c(1,3));plot(data_crime[,c(2,3,6)])

par(mfrow=c(1,3));for (i in c(3,4,5)) hist(data_crime[,i],main=names(data_crime)[i])

# Influence and potential points
# bad, crime, lawyers, employ, pop
lm_crime <- lm(expend~crime,data=data_crime)
plot(cooks.distance(lm_crime),type="b")
plot(expend,crime)

# Collinearity
round(cor(data_crime[2:7]),2)
pairs(data_crime[2:7])

```

**b)** Fit a linear regression model to the data. Use both the step-up and the step-down method to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice. 

```{r}
# bad, crime, lawyers, employ, pop
lm_crime_tot <- lm(expend~lawyers+employ,data=data_crime)
summary(lm_crime_tot)

# Step down; delete crime, delete pop, delete bad
lm_crime_tot <- lm(expend~lawyers+employ,data=data_crime)
summary(lm_crime_tot)
```


**c)** Check the model assumptions by using relevant diagnostic tools.
```{r}
par(mfrow=c(1,2))
qqnorm(residuals(lm_crime_tot))
plot(fitted(lm_crime_tot), residuals(lm_crime_tot))

par(mfrow=c(1,2))
plot(lm_crime_tot,1)
plot(lm_crime_tot,2)

```

